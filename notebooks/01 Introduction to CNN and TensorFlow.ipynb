{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to CNN and TensorFlow\n",
    "\n",
    "This tutorial gives a very brief introduction of Convolutional Neural Network (CNN) and TensorFlow. Comparing with existing tutorials on TensorFlow and CNN, which are enormous, this tutorial focuses more on the mechanism behind TensorFlow. We elaborate based on the concept of computation graph, which TensorFlow uses to define, execute, and restore computation, not only for CNN, but also for general tasks.\n",
    "\n",
    "This tutorial is organised as follows:\n",
    "\n",
    "1. [Basic TensorFlow usage](#Basic-TensorFlow-Usage)\n",
    "2. [Computaion Graph in TensorFlow](#Computation-Graph-in-TensorFlow)\n",
    "3. [CNN as computation graph](#CNN-as-Computation-Graph)\n",
    "\n",
    "Our tutorial is targeting at the **graph execution** mode of TensorFlow, in contrast to **eager execution**, which is introduced by recent TensorFlow releases to perform computation on-the-fly, without building the computation graph at first.\n",
    "\n",
    "Only the **inference** task is covered in this tutorial.\n",
    "\n",
    "In the following tutorials, we will feed the TensorFlow models we build and store here for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Basic TensorFlow Usage\n",
    "\n",
    "TensorFlow is, on the one hand, a famous machine learning framework on which developers can easily build their models, while on the other hand, a **Domain-Specific Language** (DSL) embedded in Python. As a DSL, TensorFlow provides *primitives*, or APIs, to construct computation graphs for machine learning models. Therefore, in this tutorial, we introduce TensorFlow as a programming language: we first present elements of a typical programming language from TensorFlow, such as syntax, variables, data types, literals, and operators; next, show how a program constructed by TensorFlow can be executed; in the end, there will be a short list of tips and tricks about using TensorFlow.\n",
    "\n",
    "We will look into the mechanism of computation graph in TensorFlow in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Syntax\n",
    "\n",
    "Assuming you have already installed TensorFlow (if not, please follow this [tutorial](https://www.tensorflow.org/install/)), the last step before using TensorFlow is importing it in Python, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import tensorflow and set its alias as \"tf\"\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic element you can operate on in the TensorFlow DSL is **tensor**, as documented in [tf.Tensor](https://www.tensorflow.org/api_docs/python/tf/Tensor). Informally speaking, a tensor is a *N*-dimensional array, similar to [ndarray](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.ndarray.html) in NumPy. Below is an example that shows how to build tensor from a constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const:0' shape=(2, 2) dtype=float32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([[1.0, 2.0], [3.0, 4.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides constants, you can create **variable** tensors, see this [document](https://www.tensorflow.org/programmers_guide/variables) for more details. A variable can be created with the name and shape through `get_variable`, as shown below. Tensor name should be unique in its [name scope](https://www.tensorflow.org/api_docs/python/tf/name_scope)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'a:0' shape=(2, 2) dtype=float32_ref>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.get_variable('a', [2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can build **computation** use [operators](https://www.tensorflow.org/api_docs/python/tf/Operation) specified by TensorFlow. `tf.matmul` is an intuitive example which performs matrix multiplication among two 2-D tensors (matrices). The object returned from a TensorFlow operator is a tensor that holds the operation's result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MatMul:0' shape=(2, 2) dtype=float32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(tf.constant([[1.0, 2.0], [3.0, 4.0]]), tf.constant([[1.0, 2.0], [3.0, 4.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the computation, i.e., get the content of tensors, we need to use `tf.Session` (see [here](https://www.tensorflow.org/api_docs/python/tf/Session)) to initialise an environment to execute the program constructed by TensorFlow. We will not dive into details about session.\n",
    "\n",
    "Note that each variable should be properly initialised in a session. The example below uses `random_normal_initializer` to initialse the content of tensor `b` with random values distributed in normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04607815 -0.43811703]\n",
      " [-0.70926595  1.0317197 ]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "b = tf.get_variable('b', [2, 2], initializer=tf.random_normal_initializer())\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation Graph in TensorFlow\n",
    "\n",
    "Almost every computer program can be represented in a computation graph. A program constructed by TensorFlow explicitly initialises its computation graph. The computation graph built by TensorFlow is a [dataflow graph](https://www.tensorflow.org/programmers_guide/graphs#why_dataflow_graphs), in which each node represents computation (or operation) and each edge is data. The gif below shows how the dataflow graph constructed for a two layer MLP (**M**ulti-**L**ayer **P**erceptron). In the first layer, The input data will first go through matrix multiplication (`tf.matmul`), bias vector addition (`BiasAdd`), and non-linear activation (`tf.nn.relu`). The second layer processes the output data in the same setting except the ReLU activation. Weights and bias vectors are variable tensors. Note that nodes for training (`Gradients`, `Update`) are also initialised in this graph.\n",
    "\n",
    "![](https://www.tensorflow.org/images/tensors_flowing.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access Graph Contents\n",
    "\n",
    "The purpose of this tutorial is mainly to understand the mechanism in TensorFlow for further optimisation and deployment. In this sense it is necessary to access the contents within the graph we construct. Suppose we build a graph that do exactly the same computation as the animated figure illustrates, from `Input` to `Softmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = tf.Graph() # build a new graph\n",
    "with g.as_default():\n",
    "    input_tensor = tf.placeholder(tf.float32, [28, 28, 1])\n",
    "    x = tf.reshape(input_tensor, [1, 784], name='x')\n",
    "    \n",
    "    # The first perceptron layer\n",
    "    W1 = tf.get_variable('W1', [784, 1024])\n",
    "    b1 = tf.get_variable('b1', [1024])\n",
    "    y1 = tf.nn.relu(tf.nn.bias_add(tf.matmul(x, W1), b1), name='y1')\n",
    "    # The second perceptron layer\n",
    "    W2 = tf.get_variable('W2', [1024, 10])\n",
    "    b2 = tf.get_variable('b2', [10])\n",
    "    y2 = tf.nn.bias_add(tf.matmul(y1, W2), b2, name='y2')\n",
    "    \n",
    "    logits = tf.nn.softmax(y2, name='logits') # softmax output is named as \"logits\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow puts all graph related information in the `tf.Graph` [class](https://www.tensorflow.org/api_docs/python/tf/Graph), as the object `g` in the example above. This graph object holds the definition of the graph as [`GraphDef`](https://www.tensorflow.org/api_docs/python/tf/GraphDef), which can be accessed by `as_graph_def()`. You can iterate through every node in the `GraphDef` by its `node` property. Each node has `name`, `op` for operation name, `input` for a list of names of nodes that are input to the current node, and other accessible properties. In the example below, we output the properties of every `MatMul` nodes. The syntax of node definition is [Protocal Buffers](https://developers.google.com/protocol-buffers/), please see their document for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"MatMul\"\n",
      "op: \"MatMul\"\n",
      "input: \"x\"\n",
      "input: \"W1/read\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"transpose_a\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"transpose_b\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "\n",
      "name: \"MatMul_1\"\n",
      "op: \"MatMul\"\n",
      "input: \"y1\"\n",
      "input: \"W2/read\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"transpose_a\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"transpose_b\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graph_def = g.as_graph_def()\n",
    "for node in graph_def.node:\n",
    "    if node.op == 'MatMul':\n",
    "        print(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides accessing node definitions, we can read the content of each tensor instance in the graph through the `get_tensor_by_name` method of `tf.Graph`. This method returns a `Tensor` object, and we can read its content by passing it to `sess.run`. Note that the tensor name is not exactly the corresponding node name: we need to append [device placement](https://www.tensorflow.org/programmers_guide/graphs#placing_operations_on_different_devices). In the example below, the weight `W1` is placed on the device with index `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0407966   0.01642247 -0.01267914 ... -0.0498566   0.01870576\n",
      "   0.0475702 ]\n",
      " [ 0.00524014 -0.04692878 -0.04888818 ...  0.01610609 -0.03872287\n",
      "   0.00923277]\n",
      " [-0.03734346  0.00120491 -0.02782688 ... -0.01435609 -0.02347288\n",
      "  -0.00529803]\n",
      " ...\n",
      " [-0.05257782 -0.00495375 -0.02191918 ...  0.00365885 -0.04365067\n",
      "  -0.02331771]\n",
      " [-0.03647182 -0.00234056 -0.00780445 ...  0.03683112  0.02983878\n",
      "   0.05280031]\n",
      " [-0.04986075 -0.03048103 -0.02709062 ... -0.02339984 -0.03050341\n",
      "  -0.05025662]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g) as sess: # we need to explicitly set the graph or the default graph will be used.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # read the content of initialised W1\n",
    "    W1_tensor = g.get_tensor_by_name('W1:0')\n",
    "    print(sess.run(W1_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the content of tensors that depends on the `Input` node, which is a placeholder, we need to *feed* contents to it while executing in the session. See this document: https://www.tensorflow.org/api_docs/python/tf/placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.05206797 0.03932259 0.20525938 0.02211069 0.07760996 0.17356311\n",
      "  0.10685903 0.18140034 0.1002479  0.04155892]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # read the final classification result of a random image\n",
    "    logits_tensor = g.get_tensor_by_name('logits:0')\n",
    "    print(sess.run(logits_tensor, feed_dict={input_tensor: np.random.random((28, 28, 1))}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN as Computation Graph\n",
    "\n",
    "Based on previous discussion, we can construct CNN and read its structure and coeffcients in TensorFlow. A CNN can be viewed as a special computation graph that contains **convolutional layer**, which is `tf.nn.conv2d` in TensorFlow (https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). Details in the principles of convolutional layer can be accessed in this [tutorial](http://cs231n.github.io/convolutional-networks/#conv). \n",
    "\n",
    "In a typical CNN, the input and output of each layer are **feature maps**, which are conventionally 3-D tensors (or 4-D if you use batched input). A feature map can be seen as a multi-channel image, which can represent natural image or features extracted from the latent space. The shape of each feature map is $H\\times W \\times C$, in which $H$ and $W$ are height and width of a spatial feature map, and $C$ is the number of image channels. A convolutional layer performs 2D convolution in each spatial feature map, and sum the results together for different output channels. Besides convolutional layer, max pooling layer is also important to reduce the size of feature maps to extract features at higher levels.\n",
    "\n",
    "Here we build an example CNN that can do hand-writen digits classification in TensorFlow. This CNN architecture is known as [LeNet](http://yann.lecun.com/exdb/lenet/). It should be trained on the MNIST dataset. Similar tutorials can be found at https://www.tensorflow.org/tutorials/deep_cnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lenet(images, keep_prob):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        images: a 4-D tensor that holds batched input images\n",
    "    Return:\n",
    "        A tensor that contains classification probabilities result, and a dictionary\n",
    "        of all intermediate tensors.\n",
    "    \"\"\"    \n",
    "    end_points = {}\n",
    "    end_points['images'] = tf.reshape(images, [-1, 28, 28, 1])\n",
    "\n",
    "    with tf.variable_scope('conv1'):\n",
    "        w1 = tf.get_variable('weights', [5, 5, 1, 32])\n",
    "        b1 = tf.get_variable('biases', [32],\n",
    "                             initializer=tf.zeros_initializer())\n",
    "        end_points['conv1'] = tf.nn.relu(\n",
    "            tf.nn.conv2d(end_points['images'], w1, [1, 1, 1, 1], 'SAME') + b1)\n",
    "    end_points['pool1'] = tf.nn.max_pool(\n",
    "        end_points['conv1'], [1, 2, 2, 1], [1, 2, 2, 1], 'SAME')\n",
    "    \n",
    "    with tf.variable_scope('conv2'):\n",
    "        w2 = tf.get_variable('weights', [5, 5, 32, 64])\n",
    "        b2 = tf.get_variable('biases', [64],\n",
    "                             initializer=tf.zeros_initializer())\n",
    "        end_points['conv2'] = tf.nn.relu(\n",
    "            tf.nn.conv2d(end_points['pool1'], w2, [1, 1, 1, 1], 'SAME') + b2)\n",
    "    end_points['pool2'] = tf.nn.max_pool(\n",
    "        end_points['conv2'], [1, 2, 2, 1], [1, 2, 2, 1], 'SAME')\n",
    "    \n",
    "    end_points['flatten'] = tf.reshape(end_points['pool2'], [-1, 7 * 7 * 64])\n",
    "    with tf.variable_scope('fc3'):\n",
    "        w3 = tf.get_variable('weights', [7 * 7 * 64, 1024])\n",
    "        b3 = tf.get_variable('biases', [1024],\n",
    "                             initializer=tf.zeros_initializer())\n",
    "        end_points['fc3'] = tf.nn.relu(tf.matmul(end_points['flatten'], w3) + b3)\n",
    "        \n",
    "    end_points['dropout'] = tf.nn.dropout(end_points['fc3'], keep_prob)\n",
    "    with tf.variable_scope('fc4'):\n",
    "        w4 = tf.get_variable('weights', [1024, 10])\n",
    "        b4 = tf.get_variable('biases', [10],\n",
    "                             initializer=tf.zeros_initializer())\n",
    "        end_points['fc4'] = tf.matmul(end_points['fc3'], w4) + b4\n",
    "    \n",
    "    return end_points['fc4'], end_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then train this CNN based on the MNIST dataset (reference: https://www.tensorflow.org/versions/r1.0/get_started/mnist/beginners). Below is the code snippet we use to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "Loss value of a training batch at step     0: 2.312549\n",
      "Accuracy after running     0 steps: 0.089200\n",
      "Loss value of a training batch at step   100: 2.302420\n",
      "Loss value of a training batch at step   200: 2.278491\n",
      "Loss value of a training batch at step   300: 2.283604\n",
      "Loss value of a training batch at step   400: 2.269118\n",
      "Loss value of a training batch at step   500: 2.236468\n",
      "Loss value of a training batch at step   600: 2.266208\n",
      "Loss value of a training batch at step   700: 2.260607\n",
      "Loss value of a training batch at step   800: 2.235821\n",
      "Loss value of a training batch at step   900: 2.251554\n",
      "Loss value of a training batch at step  1000: 2.222722\n",
      "Accuracy after running  1000 steps: 0.461100\n",
      "Loss value of a training batch at step  1100: 2.210543\n",
      "Loss value of a training batch at step  1200: 2.198042\n",
      "Loss value of a training batch at step  1300: 2.198376\n",
      "Loss value of a training batch at step  1400: 2.182670\n",
      "Loss value of a training batch at step  1500: 2.181722\n",
      "Loss value of a training batch at step  1600: 2.181613\n",
      "Loss value of a training batch at step  1700: 2.155800\n",
      "Loss value of a training batch at step  1800: 2.180903\n",
      "Loss value of a training batch at step  1900: 2.147303\n",
      "Loss value of a training batch at step  2000: 2.139310\n",
      "Accuracy after running  2000 steps: 0.528300\n",
      "Loss value of a training batch at step  2100: 2.179488\n",
      "Loss value of a training batch at step  2200: 2.116657\n",
      "Loss value of a training batch at step  2300: 2.136807\n",
      "Loss value of a training batch at step  2400: 2.131047\n",
      "Loss value of a training batch at step  2500: 2.094605\n",
      "Loss value of a training batch at step  2600: 2.034612\n",
      "Loss value of a training batch at step  2700: 2.093701\n",
      "Loss value of a training batch at step  2800: 2.139953\n",
      "Loss value of a training batch at step  2900: 2.116894\n",
      "Loss value of a training batch at step  3000: 2.119567\n",
      "Accuracy after running  3000 steps: 0.563200\n",
      "Loss value of a training batch at step  3100: 2.080743\n",
      "Loss value of a training batch at step  3200: 2.016922\n",
      "Loss value of a training batch at step  3300: 2.045662\n",
      "Loss value of a training batch at step  3400: 2.006077\n",
      "Loss value of a training batch at step  3500: 2.003067\n",
      "Loss value of a training batch at step  3600: 1.910141\n",
      "Loss value of a training batch at step  3700: 1.938264\n",
      "Loss value of a training batch at step  3800: 2.028977\n",
      "Loss value of a training batch at step  3900: 1.973304\n",
      "Loss value of a training batch at step  4000: 1.932342\n",
      "Accuracy after running  4000 steps: 0.664200\n",
      "Loss value of a training batch at step  4100: 1.946874\n",
      "Loss value of a training batch at step  4200: 1.963283\n",
      "Loss value of a training batch at step  4300: 1.884900\n",
      "Loss value of a training batch at step  4400: 1.892479\n",
      "Loss value of a training batch at step  4500: 1.907459\n",
      "Loss value of a training batch at step  4600: 1.817524\n",
      "Loss value of a training batch at step  4700: 1.809688\n",
      "Loss value of a training batch at step  4800: 1.837890\n",
      "Loss value of a training batch at step  4900: 1.778381\n",
      "Loss value of a training batch at step  5000: 1.891319\n",
      "Accuracy after running  5000 steps: 0.700600\n",
      "Loss value of a training batch at step  5100: 1.931512\n",
      "Loss value of a training batch at step  5200: 1.810122\n",
      "Loss value of a training batch at step  5300: 1.768922\n",
      "Loss value of a training batch at step  5400: 1.798079\n",
      "Loss value of a training batch at step  5500: 1.759528\n",
      "Loss value of a training batch at step  5600: 1.717996\n",
      "Loss value of a training batch at step  5700: 1.636284\n",
      "Loss value of a training batch at step  5800: 1.869639\n",
      "Loss value of a training batch at step  5900: 1.654586\n",
      "Loss value of a training batch at step  6000: 1.670741\n",
      "Accuracy after running  6000 steps: 0.715500\n",
      "Loss value of a training batch at step  6100: 1.738334\n",
      "Loss value of a training batch at step  6200: 1.775625\n",
      "Loss value of a training batch at step  6300: 1.734281\n",
      "Loss value of a training batch at step  6400: 1.719681\n",
      "Loss value of a training batch at step  6500: 1.608994\n",
      "Loss value of a training batch at step  6600: 1.681469\n",
      "Loss value of a training batch at step  6700: 1.648841\n",
      "Loss value of a training batch at step  6800: 1.759724\n",
      "Loss value of a training batch at step  6900: 1.516997\n",
      "Loss value of a training batch at step  7000: 1.519885\n",
      "Accuracy after running  7000 steps: 0.725000\n",
      "Loss value of a training batch at step  7100: 1.505617\n",
      "Loss value of a training batch at step  7200: 1.478342\n",
      "Loss value of a training batch at step  7300: 1.481374\n",
      "Loss value of a training batch at step  7400: 1.396559\n",
      "Loss value of a training batch at step  7500: 1.558221\n",
      "Loss value of a training batch at step  7600: 1.673242\n",
      "Loss value of a training batch at step  7700: 1.420385\n",
      "Loss value of a training batch at step  7800: 1.506977\n",
      "Loss value of a training batch at step  7900: 1.463950\n",
      "Loss value of a training batch at step  8000: 1.337660\n",
      "Accuracy after running  8000 steps: 0.735100\n",
      "Loss value of a training batch at step  8100: 1.534414\n",
      "Loss value of a training batch at step  8200: 1.634570\n",
      "Loss value of a training batch at step  8300: 1.542301\n",
      "Loss value of a training batch at step  8400: 1.348242\n",
      "Loss value of a training batch at step  8500: 1.353656\n",
      "Loss value of a training batch at step  8600: 1.193262\n",
      "Loss value of a training batch at step  8700: 1.380915\n",
      "Loss value of a training batch at step  8800: 1.277076\n",
      "Loss value of a training batch at step  8900: 1.320735\n",
      "Loss value of a training batch at step  9000: 1.309686\n",
      "Accuracy after running  9000 steps: 0.740600\n",
      "Loss value of a training batch at step  9100: 1.409944\n",
      "Loss value of a training batch at step  9200: 1.351443\n",
      "Loss value of a training batch at step  9300: 1.373128\n",
      "Loss value of a training batch at step  9400: 1.334326\n",
      "Loss value of a training batch at step  9500: 1.187030\n",
      "Loss value of a training batch at step  9600: 1.038742\n",
      "Loss value of a training batch at step  9700: 1.332207\n",
      "Loss value of a training batch at step  9800: 1.207648\n",
      "Loss value of a training batch at step  9900: 1.191213\n",
      "Loss value of a training batch at step 10000: 1.307708\n",
      "Accuracy after running 10000 steps: 0.747500\n",
      "Loss value of a training batch at step 10100: 1.236033\n",
      "Loss value of a training batch at step 10200: 1.305236\n",
      "Loss value of a training batch at step 10300: 1.120224\n",
      "Loss value of a training batch at step 10400: 1.060395\n",
      "Loss value of a training batch at step 10500: 1.020534\n",
      "Loss value of a training batch at step 10600: 1.190458\n",
      "Loss value of a training batch at step 10700: 1.084669\n",
      "Loss value of a training batch at step 10800: 0.998269\n",
      "Loss value of a training batch at step 10900: 1.268621\n",
      "Loss value of a training batch at step 11000: 1.319499\n",
      "Accuracy after running 11000 steps: 0.753000\n",
      "Loss value of a training batch at step 11100: 1.120054\n",
      "Loss value of a training batch at step 11200: 1.123228\n",
      "Loss value of a training batch at step 11300: 1.239749\n",
      "Loss value of a training batch at step 11400: 1.094772\n",
      "Loss value of a training batch at step 11500: 0.988068\n",
      "Loss value of a training batch at step 11600: 1.125580\n",
      "Loss value of a training batch at step 11700: 0.996747\n",
      "Loss value of a training batch at step 11800: 0.888635\n",
      "Loss value of a training batch at step 11900: 0.888054\n",
      "Loss value of a training batch at step 12000: 1.127748\n",
      "Accuracy after running 12000 steps: 0.760600\n",
      "Loss value of a training batch at step 12100: 0.992315\n",
      "Loss value of a training batch at step 12200: 1.004344\n",
      "Loss value of a training batch at step 12300: 1.085054\n",
      "Loss value of a training batch at step 12400: 0.988526\n",
      "Loss value of a training batch at step 12500: 0.809230\n",
      "Loss value of a training batch at step 12600: 1.021914\n",
      "Loss value of a training batch at step 12700: 0.999088\n",
      "Loss value of a training batch at step 12800: 1.002370\n",
      "Loss value of a training batch at step 12900: 1.226260\n",
      "Loss value of a training batch at step 13000: 1.090428\n",
      "Accuracy after running 13000 steps: 0.767800\n",
      "Loss value of a training batch at step 13100: 1.026530\n",
      "Loss value of a training batch at step 13200: 1.126817\n",
      "Loss value of a training batch at step 13300: 1.248782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss value of a training batch at step 13400: 0.883835\n",
      "Loss value of a training batch at step 13500: 0.975058\n",
      "Loss value of a training batch at step 13600: 0.862852\n",
      "Loss value of a training batch at step 13700: 0.994683\n",
      "Loss value of a training batch at step 13800: 0.858009\n",
      "Loss value of a training batch at step 13900: 0.919513\n",
      "Loss value of a training batch at step 14000: 0.757377\n",
      "Accuracy after running 14000 steps: 0.773300\n",
      "Loss value of a training batch at step 14100: 0.814935\n",
      "Loss value of a training batch at step 14200: 0.982731\n",
      "Loss value of a training batch at step 14300: 0.859649\n",
      "Loss value of a training batch at step 14400: 0.862044\n",
      "Loss value of a training batch at step 14500: 1.084536\n",
      "Loss value of a training batch at step 14600: 0.715158\n",
      "Loss value of a training batch at step 14700: 0.850026\n",
      "Loss value of a training batch at step 14800: 0.808401\n",
      "Loss value of a training batch at step 14900: 0.756195\n",
      "Loss value of a training batch at step 15000: 1.079442\n",
      "Accuracy after running 15000 steps: 0.781200\n",
      "Loss value of a training batch at step 15100: 0.829812\n",
      "Loss value of a training batch at step 15200: 0.918013\n",
      "Loss value of a training batch at step 15300: 0.810382\n",
      "Loss value of a training batch at step 15400: 0.749857\n",
      "Loss value of a training batch at step 15500: 0.931000\n",
      "Loss value of a training batch at step 15600: 0.789123\n",
      "Loss value of a training batch at step 15700: 0.903766\n",
      "Loss value of a training batch at step 15800: 0.951269\n",
      "Loss value of a training batch at step 15900: 0.889873\n",
      "Loss value of a training batch at step 16000: 0.779973\n",
      "Accuracy after running 16000 steps: 0.784800\n",
      "Loss value of a training batch at step 16100: 0.771583\n",
      "Loss value of a training batch at step 16200: 1.057010\n",
      "Loss value of a training batch at step 16300: 0.996317\n",
      "Loss value of a training batch at step 16400: 0.908952\n",
      "Loss value of a training batch at step 16500: 0.709163\n",
      "Loss value of a training batch at step 16600: 0.960513\n",
      "Loss value of a training batch at step 16700: 0.628846\n",
      "Loss value of a training batch at step 16800: 0.945502\n",
      "Loss value of a training batch at step 16900: 0.834948\n",
      "Loss value of a training batch at step 17000: 0.787408\n",
      "Accuracy after running 17000 steps: 0.790400\n",
      "Loss value of a training batch at step 17100: 0.921081\n",
      "Loss value of a training batch at step 17200: 0.714092\n",
      "Loss value of a training batch at step 17300: 0.923466\n",
      "Loss value of a training batch at step 17400: 0.598847\n",
      "Loss value of a training batch at step 17500: 0.631971\n",
      "Loss value of a training batch at step 17600: 1.007187\n",
      "Loss value of a training batch at step 17700: 0.470473\n",
      "Loss value of a training batch at step 17800: 0.791756\n",
      "Loss value of a training batch at step 17900: 0.781758\n",
      "Loss value of a training batch at step 18000: 0.629119\n",
      "Accuracy after running 18000 steps: 0.795300\n",
      "Loss value of a training batch at step 18100: 0.684141\n",
      "Loss value of a training batch at step 18200: 0.767897\n",
      "Loss value of a training batch at step 18300: 0.737905\n",
      "Loss value of a training batch at step 18400: 0.896366\n",
      "Loss value of a training batch at step 18500: 0.550920\n",
      "Loss value of a training batch at step 18600: 0.653963\n",
      "Loss value of a training batch at step 18700: 0.993648\n",
      "Loss value of a training batch at step 18800: 0.838637\n",
      "Loss value of a training batch at step 18900: 0.945296\n",
      "Loss value of a training batch at step 19000: 0.775147\n",
      "Accuracy after running 19000 steps: 0.799400\n",
      "Loss value of a training batch at step 19100: 0.775871\n",
      "Loss value of a training batch at step 19200: 0.541332\n",
      "Loss value of a training batch at step 19300: 0.820676\n",
      "Loss value of a training batch at step 19400: 0.934875\n",
      "Loss value of a training batch at step 19500: 0.545093\n",
      "Loss value of a training batch at step 19600: 0.752811\n",
      "Loss value of a training batch at step 19700: 0.538812\n",
      "Loss value of a training batch at step 19800: 0.894607\n",
      "Loss value of a training batch at step 19900: 0.495215\n"
     ]
    }
   ],
   "source": [
    "# NOTE: You don't need to run this code snippet since we have already trained it\n",
    "# and it will consume lots of resources on our server.\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    images = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    labels = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    logits, end_points = lenet(images, keep_prob)\n",
    "    \n",
    "    # Nodes for training\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits)\n",
    "    train = tf.train.AdadeltaOptimizer(1e-3).minimize(loss)\n",
    "    \n",
    "    # accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session(graph=g) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for i in range(20000):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(50)\n",
    "            _, loss_val = sess.run([train, loss],\n",
    "                                   feed_dict={images: batch_xs,\n",
    "                                              labels: batch_ys,\n",
    "                                              keep_prob: 0.5})\n",
    "        \n",
    "            if i % 100 == 0:\n",
    "                print('Loss value of a training batch at step %5d: %f' % (i, np.mean(loss_val)))\n",
    "            if i % 1000 == 0:\n",
    "                acc = sess.run(accuracy,\n",
    "                               feed_dict={images: mnist.test.images,\n",
    "                                          labels: mnist.test.labels,\n",
    "                                          keep_prob: 1.0})\n",
    "                print('Accuracy after running %5d steps: %f' % (i, acc))\n",
    "        \n",
    "        # save the trained model\n",
    "        saver.save(sess, \"mnist_lenet_log/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise\n",
    "\n",
    "In this exercise, you are encouraged to visualize the intermediate feature maps after each **convolutional** layer and **max-pooling** layer. We build the skeleton in the following code snippet. You should finish the following tasks:\n",
    "\n",
    "1. Correctly identify the `end_point` tensors for all convolutional layers and max-pooling layers. These tensors can be viewed as the outputs of those layers.\n",
    "2. Collect the values of these tensors based on an input test image. You can select an image from the test dataset (`mnist.test.images`).\n",
    "3. For each tensor value, you can get the 2D image of any channel of the tensor. Hint: a tensor value is a NumPy `ndarray`.\n",
    "4. Visualize the 2D image by `plt.imshow` (https://matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_lenet_log/: directory\r\n"
     ]
    }
   ],
   "source": [
    "# Make sure that mnist_lenet_log, the directory contains trained model, exists.\n",
    "!file mnist_lenet_log/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXERCISE VERSION\n",
    "def visualize_tensor(image, key, channel_idx, axis):\n",
    "    \"\"\"\n",
    "    Visualize a tensor in the trained LeNet model.\n",
    "    Args:\n",
    "        image: a test image\n",
    "        key: the key to the tensor in end_points\n",
    "        channel_idx: index of the channel to be visualized\n",
    "        axis: a pyplot Axis object\n",
    "    \"\"\"\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    with g.as_default():\n",
    "        images = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "        labels = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        logits, end_points = lenet(images, keep_prob)\n",
    "\n",
    "        # Nodes for training\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits)\n",
    "        train = tf.train.AdadeltaOptimizer(1e-3).minimize(loss)\n",
    "\n",
    "        # accuracy\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        with tf.Session(graph=g) as sess:\n",
    "            saver.restore(sess, 'mnist_lenet_log/')\n",
    "            \n",
    "            # TODO: finish the line to get the tensor value of end_points[key]\n",
    "            tensor_val = sess.run(\"\"\"BLANK\"\"\", feed_dict={images: [image], keep_prob: 1.0})\n",
    "            \n",
    "            # TODO: get the 2D image at channel \"channel_idx\"\n",
    "            image_2d = tensor_val[0, \"\"\"BLANK\"\"\"]\n",
    "            \n",
    "            # TODO: visualize\n",
    "            axis.set_title(key)\n",
    "            axis.imshow(\"\"\"BLANK\"\"\", cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe4AAAEzCAYAAAD3t+CnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe91a7e5c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: use visualize_tensor to visualize the channel 0 of all convolutional layers and max-pooling layers of the first test image in MNIST\n",
    "fig, ax = plt.subplots(ncols=3, nrows=2, figsize=(8, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER VERSION\n",
    "\n",
    "def visualize_tensor(image, key, channel_idx, axis):\n",
    "    \"\"\"\n",
    "    Visualize a tensor in the trained LeNet model.\n",
    "    Args:\n",
    "        image: a test image\n",
    "        key: the key to the tensor in end_points\n",
    "        channel_idx: index of the channel to be visualized\n",
    "        axis: a pyplot Axis object\n",
    "    \"\"\"\n",
    "    g = tf.Graph()\n",
    "    with g.as_default():\n",
    "        images = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "        labels = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        logits, end_points = lenet(images, keep_prob)\n",
    "\n",
    "        # Nodes for training\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits)\n",
    "        train = tf.train.AdadeltaOptimizer(1e-3).minimize(loss)\n",
    "\n",
    "        # accuracy\n",
    "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session(graph=g) as sess:\n",
    "            saver.restore(sess, 'mnist_lenet_log/')\n",
    "            \n",
    "            # TODO: finish the line to get the tensor value of end_points[key]\n",
    "            tensor_val = sess.run(end_points[key], feed_dict={images: [image], keep_prob: 1.0})\n",
    "            \n",
    "            # TODO: get the 2D image at channel \"channel_idx\"\n",
    "            image_2d = tensor_val[0, :, :, channel_idx]\n",
    "            \n",
    "            # TODO: visualize\n",
    "            axis.set_title(key)\n",
    "            axis.imshow(image_2d, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from mnist_lenet_log/\n",
      "INFO:tensorflow:Restoring parameters from mnist_lenet_log/\n",
      "INFO:tensorflow:Restoring parameters from mnist_lenet_log/\n",
      "INFO:tensorflow:Restoring parameters from mnist_lenet_log/\n",
      "INFO:tensorflow:Restoring parameters from mnist_lenet_log/\n"
     ]
    }
   ],
   "source": [
    "# TODO: visualize image\n",
    "fig, ax = plt.subplots(ncols=3, nrows=2, figsize=(10, 5))\n",
    "\n",
    "visualize_tensor(mnist.test.images[0], 'images', 0, ax[0, 0])\n",
    "visualize_tensor(mnist.test.images[0], 'conv1', 0, ax[0, 1])\n",
    "visualize_tensor(mnist.test.images[0], 'pool1', 0, ax[0, 2])\n",
    "visualize_tensor(mnist.test.images[0], 'conv2', 0, ax[1, 0])\n",
    "visualize_tensor(mnist.test.images[0], 'pool2', 0, ax[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
